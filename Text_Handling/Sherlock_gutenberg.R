

##################################### Topic Modeling - STM ##################################### 

# source: https://www.youtube.com/watch?v=evTuL-RcRpc


library(tidyverse)
library(tidytext)
library(gutenbergr)



# get the data ------------------------------------------------------------

sherlock_raw <- gutenberg_download(gutenberg_id = 1661)

# take a look
sherlock_raw %>% 
  View()
# looks quite messy..
# so let's clean up!



# cleaning up -------------------------------------------------------------

# we know, that there are 12 short stories in the data set
# each new story begins with a greek number
# NOTE: I needs to be limited! Otherwise there are too many matches!
# separate statement needed!
greek_numbers <- c("II. |III. |IV. |V. |VI. |VII. |VIII. |IX. |X. |XI. |XII. ")

sherlock <- sherlock_raw %>% 
  mutate(story = ifelse(test = str_detect(text, pattern = greek_numbers),
                        yes = text, 
                        no = NA)) %>% 
  mutate(story = case_when(str_detect(text, "A SCANDAL IN BOHEMIA|A Scandal in Bohemia") ~ text,
                           TRUE ~ story)) %>% 
  # fill it up!
  fill(story) %>%
  # get rid of the preface
  slice(29:nrow(sherlock_raw))




# create tokens -----------------------------------------------------------

tidy_sherlock <- sherlock %>% 
  mutate(line = row_number()) %>% 
  unnest_tokens(output = word, 
                input = text,
                token = "words") %>% 
  anti_join(stop_words) %>% 
  # appears to often => has a big impact on the STM
  # filter out...
  filter(word != "holmes")


# most common words
tidy_sherlock %>% 
  count(word, sort = TRUE)





# explore tf-idf-scores ---------------------------------------------------

# what are the most important words in each story (tf idf-scores)!
# => words that are characteristic for each story
tidy_sherlock %>% 
  count(story, word, sort = TRUE) %>% 
  bind_tf_idf(term = word,
              document = story,
              n = n) %>% 
  group_by(story) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = story)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ story, scales = "free") + 
  coord_flip()




# run a stm ---------------------------------------------------------------

library(quanteda)
library(stm)

# create dfm
sherlock_dfm <- tidy_sherlock %>% 
  count(story, word, sort = TRUE) %>% 
  cast_dfm(document = story,
           term = word, 
           value = n)


# K is the result of an trial and error process
# unsupervised machined learning
topic_model <- stm(sherlock_dfm,
                   K = 6,
                   init.type = "Spectral")

summary(topic_model)




# tidy the model ----------------------------------------------------------

# what is the probability of a word beeing generated by a topic?
# Or which words are contributing the most to each topic.
tidy_beta <- tidy(topic_model,
                  matrix = "beta")

tidy_beta %>% 
  group_by(topic) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(term = fct_reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = as_factor(topic))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


# gamma = what is the probalility for a story to belong to a topic?
tidy_gamma <- tidy(topic_model,
                   matrix = "gamma",
                   document_names = rownames(sherlock_dfm))

tidy_gamma %>% 
  ggplot(aes(gamma, fill = as_factor(topic))) +
  geom_histogram(show.legend = FALSE) + 
  facet_wrap(~topic, ncol = 3)
# 2 stories belong to topic 1. The not!
# 1 story belongs to topic 2. The others not!

