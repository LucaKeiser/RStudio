---
title: "Wine Ratings"
author: "Luca"
date: "17/12/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

load packages

```{r}
library(tidyverse)
library(tidytuesdayR)
library(scales)

theme_set(theme_light())
```

Further Details:
When the modeling was performed with na.action = "na.omit" (as is the typical default), rows with NA in the initial data are omitted entirely from the augmented data frame. When the modeling was performed with na.action = "na.exclude", one should provide the original data as a second argument, at which point the augmented data will contain those rows (typically with NAs in place of the new columns). If the original data is not provided to augment() and na.action = "na.exclude", a warning is raised and the incomplete rows are dropped.


## Get the data

```{r}
wine_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv")

wine_ratings

# view(wine_ratings)
```


### Clean up

```{r}
wine_ratings_clean <- wine_ratings %>% 
  select(-`...1`) %>% 
  # pull out the year with a regular expression!
  
  # 1. attempt:
  #extract(title, "year", "(\\d\\d\\d\\d)", 
  
  # 2. attempt (fist digits has to be 1 or 2; second digit has to be 9 or 0):
  # extract(title, "year", "([12][90]\\d\\d)",
  
  # 3. attempt (just take the years > 2000)
  extract(title, "year", "(20\\d\\d)", 
         convert = TRUE,
         remove = FALSE)  %>% 
  mutate(year = ifelse(year < 1900, NA, year)) %>%
  filter(!is.na(price))
```


## Take a Look at the Distributions first

```{r}
wine_ratings_clean %>% 
  count(country, sort = TRUE)

wine_ratings_clean %>% 
  count(designation, sort = TRUE)

wine_ratings_clean %>% 
  count(country, region_1, sort = TRUE)

wine_ratings_clean %>% 
  count(taster_name, sort = TRUE)

wine_ratings_clean %>% 
  count(variety, sort = TRUE)

wine_ratings_clean %>% 
  filter(!is.na(designation)) %>% 
  count(designation, variety, sort = TRUE)


# some data error => we need to fix the regular expression!
wine_ratings_clean %>% 
  ggplot(aes(year)) + 
  geom_histogram()


wine_ratings_clean %>% 
  ggplot(aes(points)) +
  geom_histogram(binwidth = 1)

# if we want to predict the price we must take the log(price)!
wine_ratings_clean %>% 
  ggplot(aes(price)) +
  geom_histogram() +
  scale_x_log10()
```



############################### Linear Model ###############################

## NOTE: if we create a model, the NAs are automatically dropped!
## This will be important if we try to visualize the model with the
## broom-package!

1) Price
```{r}
## log(price)
ggplot(wine_ratings_clean, aes(price, points))+ 
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm") +
  scale_x_log10()


summary(lm(points ~ log2(price), data = wine_ratings_clean))
# every time the price doubles, the expected numer of points goes up by 2!
# clear linear trend

```


2) Country

```{r}

# box plots
# NOTE: this does not (!) control for price! Our linear model above does (c.p.!)!
wine_ratings_clean %>% 
  
  # country has to many levels => fct_lump!
  # set US as reference category => compared to the US!
  # NOTE: the base-level does not have any effect on the predictions just changes
  # to which country the numbers are relative to!
  # what is the most reasonable reference category?
  mutate(country = fct_lump(country, 7),
         country = fct_relevel(country, "US"),
         country = fct_reorder(country, points)) %>% 
  
  ggplot(aes(country, points)) +
    geom_boxplot()


# are these differences significant or just due to change?
# => include coutnry in the model!
wine_ratings_clean %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  lm(points ~ log2(price) + country, data = .) %>% 
  summary()

# all p-values are significant
```



3) Year

```{r}
wine_ratings_clean %>% 
  ggplot(aes(year, points, group = year)) +
  geom_boxplot()

# OR
wine_ratings_clean %>% 
  group_by(year) %>% 
  summarise(points = mean(points)) %>% 
  ggplot(aes(year, points)) +
  geom_line()


wine_ratings_clean %>% 
  group_by(year) %>% 
  count()
# 2017 just ha 11 observations => could be noise!


# include year as a predictor (compare R-squared-values!)
wine_ratings_clean %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US")) %>% 
  lm(points ~ log2(price) + country + year, data = .) %>% 
  summary()

# all p-values are significant
```


4) Taster Name

```{r}
wine_ratings_clean %>% 
  mutate(taster_name = fct_lump(taster_name, 10),
         taster_name = fct_reorder(taster_name, points)) %>% 
  ggplot(aes(taster_name, points)) +
  geom_boxplot() +
  coord_flip()


wine_ratings_clean %>% 
  replace_na(list(taster_name = "Missing", country = "Missing")) %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US"),
         taster_name = fct_relevel(fct_lump(taster_name, 6), "Missing")) %>% 
  lm(points ~ log2(price) + country + year + taster_name, data = .)


```


Broom-Package to Visualize the model

```{r}
# we have a lot of factors => plot them in a good way
# use broom => converts statistical objects into tidy tibbles!
# coefficient-plot!
library(broom)



############################# SIDENOTE ############################# 

# lm => removes NAs automatically!
# original
wine_ratings_clean %>% 
  nrow()

# model-setting
wine_ratings_clean %>% 
  replace_na(list(taster_name = "Missing")) %>% 
  select(points, price, country, year, taster_name) %>% 
  drop_na() %>% 
  nrow()

# difference: 5879 cases

# broom::augment() throws an error:
# Error: Assigned data `predict(x, na.action = na.pass, ...) %>% unname()` must be 
# compatible with existing data.
# x Existing data has 120975 rows.
# x Assigned data has 115096 rows.
# i Only vectors of size 1 are recycled.

# so we first need to take care of the NAs!
# here we just drop them...

# create separate tibble
wine_ratings_clean_no_NAs <- wine_ratings_clean %>% 
  replace_na(list(taster_name = "Missing")) %>% 
  select(points, price, country, year, taster_name) %>% 
  drop_na()

wine_ratings_clean_no_NAs


# create the model again and store it as object...
model <- wine_ratings_clean_no_NAs %>% 
  replace_na(list(taster_name = "Missing", country = "Missing")) %>% 
  mutate(country = fct_relevel(fct_lump(country, 7), "US"),
         taster_name = fct_relevel(fct_lump(taster_name, 6), "Missing")) %>% 
  lm(points ~ log2(price) + country + year + taster_name, data = .) 

####################################################################



model %>% 

  # use the broom-package
  # include the confidence-interval
  broom::tidy(conf.int = TRUE,
              conf.level = 0.95) %>% 
  
  # remove the intercept
  filter(term != "(Intercept)") %>% 
  # cleaning the names
  mutate(term = str_replace(term, "country", "Country: "),
         term = str_replace(term, "taster_name", "Taster: "),
         term = str_replace(term, "year", "Year"),
         term = fct_reorder(term, estimate)) %>% 
  
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))

# price and country = Spain have an high positive impact on the points
# if Michael Schnacher rates the wine => negative impact on the points
# etc.


# visualize the R^2
# add the predictions with broom::augment()
model %>%   
  broom::augment(data = wine_ratings_clean_no_NAs) %>% 
  
  # predicted points and actual points
  ggplot(aes(.fitted, points)) +
  geom_point(alpha = 0.1)

# the model explains some of the variance...


# wich predictor has the highest explanatory power concerning the variance?
stats::anova(model) %>% 
  broom::tidy() %>% 
  mutate(proportion = sumsq / sum(sumsq)) %>% 
  arrange(desc(proportion))
# most of the variance is still not explained (residuals...)
# price explains a lot of the variance!
```




############################### Lasso Regression ###############################

## Lasso regression on words in the description


```{r}
library(tidytext)

wine_ratings_words <- wine_ratings_clean %>% 
  mutate(wine_id = row_number()) %>% 
  # every word is transformed into an observation
  unnest_tokens(word, description) %>% 
  # get rid of stopwords
  anti_join(stop_words, by = "word") %>% 
  # remove some more stop words
  filter(!word %in% c("wine", "drink"),
         str_detect(word, "[a-z]"))

wine_ratings_words %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```


## which words appear togther a lot?

```{r}
library(widyr)

wine_ratings_words_filtered <- wine_ratings_words %>% 
  dplyr::distinct(wine_id, word)%>% 
  add_count(word) %>% 
  filter(n >= 1000)

wine_ratings_words_filtered %>% 
  pairwise_cor(word, wine_id, sort = TRUE)

# "verdot" and "petit" appears a lot (almost always...) together etc.


wine_ratings_words_filtered %>% 
  count(word)
```

# make a matrix out of the tibble
```{r}
wine_ratings_words_matrix <- wine_ratings_words_filtered %>% 
  cast_sparse(wine_id, word)

dim(wine_ratings_words_matrix)
# one row for each wine, one column for each word!

wine_ids <- as.integer(rownames(wine_ratings_words_matrix))
points <- wine_ratings_clean$points[wine_ids]

length(wine_ids) == length(points)

```

# Penalised linear model (glmnet())
Penalised linear models, e.g. glmnet::glmnet(), add a penalty term to the distance that penalises complex models (as defined by the distance between the parameter vector and the origin). This tends to make models that generalise better to new datasets from the same population.
Sparse regression because we have a lot of predictors! Risk of over fitting is high...

```{r}
library(glmnet)

glmnet_model <- glmnet(wine_ratings_words_matrix, points)

glmnet_model %>% 
  tidy() %>% 
  filter(term %in% c("rich", "black", "simple",
                     "complex", "vineyard", "concentraded")) %>% 
  ggplot(aes(lambda, estimate, color = term)) + 
  geom_line() +
  # always use log10 on the x.axis!
  scale_x_log10() +
  geom_hline(lty = 2, yintercept = 0)

# with this graph we can visualize the change in the predictors (here: words)
# when we add more and more predictors! They look preatty stable.
# lambda gets smaller the more predictors are added (penalizes complex models)
# risk of over fitting gets bigger the more predictors are added

# see how lambda changes when we add more predictors
glmnet_model %>% 
  broom::tidy() %>% 
  count(lambda) %>% 
  ggplot(aes(lambda, n)) +
  geom_line() +
  scale_x_log10()

# what is a good lambda? Or how do we pick lambda?
# cross-validation!
# parallel processing with doSNOW
library(doSNOW)
ncores <- parallel::detectCores()
ctemp <- makeCluster(ncores-1)
registerDoSNOW(ctemp)

cv.glmnet_model <- cv.glmnet(wine_ratings_words_matrix, points, parallel = TRUE)
plot(cv.glmnet_model)
# we do not over fit the model => predictive power of the model still increases => MSE gets smaller (simple: the squared mean distance between the prediction and the actual value gets smaller)
# if we would see a sort of U-shape => indication of over fitting the model
```

Note: so far we have not included the price although the price has a high impact!
=> include the price

```{r}
wine_ratings_words_matrix_extra <- cbind(wine_ratings_words_matrix, log_price = log2(wine_ratings_clean$price[wine_ids]))

dim(wine_ratings_words_matrix)
dim(wine_ratings_words_matrix_extra)

table(is.na(wine_ratings_words_matrix_extra[ , "log_price"]))
# good!

# refit the model
cv.glmnet_model <- cv.glmnet(wine_ratings_words_matrix_extra, points, parallel = TRUE)
plot(cv.glmnet_model)

# let's choose a good lambda 
# lambda.1se = more conservative value
cv.glmnet_model$lambda.1se

cv.glmnet_model$glmnet.fit %>% 
  tidy() %>% 
  filter(lambda == cv.glmnet_model$lambda.1se,
         term != "(Intercept)") %>% 
  arrange(desc(estimate))

# which word have the highest positive impact on the points?
# log_price has the highest positive impact than beautiful etc.


# coefficient-plot
cv.glmnet_model$glmnet.fit %>% 
  tidy() %>% 
  filter(lambda == cv.glmnet_model$lambda.1se,
         term != "(Intercept)",
         # we are interested in the words...
         term != "log_price") %>% 
  arrange(estimate) %>% 
  # get the 10 most positive and negative words
  group_by(direction = ifelse(estimate < 0, "Negative", "Positive")) %>% 
  top_n(10, abs(estimate)) %>% 
  ungroup() %>% 
  mutate(term = fct_reorder(term, estimate)) %>% 
  ggplot(aes(term, estimate, fill = direction)) +
  geom_col() + 
  coord_flip() + 
  labs(title = "What Words are predictive of a Wine's Score",
       x = "Word",
       y = "Estimated Effect",
       fill = "")
```




# stop the doSNOW-cluster
stopCluster(cl = ctemp)

















